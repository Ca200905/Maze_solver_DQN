# Maze_solver_DQN
# ğŸ§  Deep Q-Learning Maze Solver

This project implements a Deep Q-Learning agent to solve a randomly generated maze. The agent uses a fully connected neural network (`fc_nn`) to approximate Q-values for navigation decisions.

---

## ğŸš€ Features

- ğŸ” Deep Q-Learning with experience replay
- ğŸ² Epsilon-decay for exploration vs. exploitation
- ğŸ® Softmax and epsilon-greedy action selection
- ğŸ“Š Visualizations for:
  - Epsilon decay
  - Policy map
  - Loss trends
  - Reward heatmap
- ğŸ§  Agent learns from raw maze state
---

## âš™ï¸ How It Works

1. **Maze Generation**  
   A random 2D maze is generated using DFS backtracking, saved as a NumPy array.

2. **Environment**  
   - Agent interacts with the maze via four actions (up/down/left/right).
   - Invalid moves penalized, goal rewarded.

3. **Agent (DQN)**  
   - Stores experiences in a replay buffer.
   - Selects actions using either softmax or Îµ-greedy.
   - Learns Q-values using MSE loss.

4. **Training Loop**  
   - Trains for a defined number of epochs.
   - Periodically saves best-performing model.
   - Visualizations generated for policy, loss, and exploration.

---
